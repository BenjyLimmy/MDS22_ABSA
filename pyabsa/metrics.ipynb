{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "{\n",
      "    \"aspect_sentiment\": {\n",
      "        \"precision\": 0.551829268292683,\n",
      "        \"recall\": 0.5397614314115308,\n",
      "        \"f1\": 0.5457286432160804\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads test data and results data from two JSONL files.\n",
    "    Skips lines that cannot be parsed as valid JSON.\n",
    "    \"\"\"\n",
    "    test_file_path = \"../datasets/laptop_quad_test.tsv.jsonl\"\n",
    "    results_file_path = \"../datasets/clean_full_results.jsonl\"\n",
    "    \n",
    "    test_data = []\n",
    "    results_data = []\n",
    "    \n",
    "    # Load test data\n",
    "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Skip empty lines\n",
    "                continue\n",
    "            try:\n",
    "                data_item = json.loads(line)\n",
    "                test_data.append(data_item)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that aren't valid JSON\n",
    "                continue\n",
    "            \n",
    "    # Load results data\n",
    "    with open(results_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Skip empty lines\n",
    "                continue\n",
    "            try:\n",
    "                data_item = json.loads(line)\n",
    "                results_data.append(data_item)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that aren't valid JSON\n",
    "                continue\n",
    "            \n",
    "    return test_data, results_data\n",
    "\n",
    "def evaluate_aspect_and_sentiment(test_data, results_data):\n",
    "    \"\"\"\n",
    "    Evaluates aspect-sentiment pairs for precision, recall, and F1 score.\n",
    "    Skips any entries that are not dictionaries or do not have a valid 'labels' list.\n",
    "    \n",
    "    Debug statements print the test_label_set and result_label_set for every comparison.\n",
    "    \"\"\"\n",
    "    total_test_labels = 0\n",
    "    total_result_labels = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # Loop over both datasets in parallel\n",
    "    for test_item, result_item in zip(test_data, results_data):\n",
    "        # Ensure both items are dictionaries\n",
    "        if not isinstance(test_item, dict) or not isinstance(result_item, dict):\n",
    "            continue\n",
    "        \n",
    "        # Extract 'labels' as lists (skip if not a list)\n",
    "        test_labels = test_item.get(\"labels\", [])\n",
    "        result_labels = result_item.get(\"labels\", [])\n",
    "        if not isinstance(test_labels, list) or not isinstance(result_labels, list):\n",
    "            continue\n",
    "        \n",
    "        # Build sets of (aspect, polarity) from test data\n",
    "        test_label_set = set()\n",
    "        for label in test_labels:\n",
    "            if isinstance(label, dict):\n",
    "                aspect = label.get(\"aspect\")\n",
    "                polarity = label.get(\"polarity\")\n",
    "                test_label_set.add((aspect, polarity))\n",
    "        \n",
    "        # Build sets of (aspect, polarity) from results data\n",
    "        result_label_set = set()\n",
    "        for label in result_labels:\n",
    "            if isinstance(label, dict):\n",
    "                aspect = label.get(\"aspect\")\n",
    "                polarity = label.get(\"polarity\")\n",
    "                result_label_set.add((aspect, polarity))\n",
    "\n",
    "        # Tally up counts\n",
    "        total_test_labels += len(test_label_set)\n",
    "        total_result_labels += len(result_label_set)\n",
    "        total_correct += len(test_label_set.intersection(result_label_set))\n",
    "    \n",
    "    # Compute precision, recall, F1\n",
    "    precision = total_correct / total_result_labels if total_result_labels else 0.0\n",
    "    recall = total_correct / total_test_labels if total_test_labels else 0.0\n",
    "    f1 = (\n",
    "        (2 * precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"aspect_sentiment\": {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def run_evaluation():\n",
    "    # Load data\n",
    "    test_data, results_data = load_data()\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_aspect_and_sentiment(test_data, results_data)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(json.dumps(metrics, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_evaluation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
