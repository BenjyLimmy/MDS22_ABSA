{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Testing Metrics - Previous Best - DeepSeek-R1-Distill-Qwen #####\n",
      "Aspect: (\n",
      "   'Precision': 0.7493\n",
      "   'Recall': 0.7343\n",
      "   'F1': 0.7329\n",
      "   'Accuracy': 0.7343\n",
      ")\n",
      "Sentiment: (\n",
      "   'Precision': 0.7860\n",
      "   'Recall': 0.8280\n",
      "   'F1': 0.7767\n",
      "   'Accuracy': 0.8280\n",
      ")\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sentiment evaluation is performed considering all predicted sentiments regardless of the wrong/correct aspects \n",
    "\n",
    "def evaluate_aspect_and_sentiment(predictions_list, ground_truth_list):\n",
    "    \"\"\"\n",
    "    Evaluates aspect and sentiment analysis results on a per-review basis, then averages the metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions_list: A list of lists, where each inner list contains tuples of (aspect, sentiment, ...).\n",
    "                            Each inner list represents the predictions for a single review.\n",
    "        ground_truth_list: A list of lists, where each inner list contains tuples of (aspect, sentiment, ...).\n",
    "                            Each inner list represents the ground truth for a single review.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the averaged aspect and sentiment evaluation metrics.\n",
    "\n",
    "    Functionality:\n",
    "        1.  Iterates through each review's predictions and ground truth.\n",
    "        2.  Calculates aspect metrics (precision, recall, F1, accuracy) for each review.\n",
    "        3.  Calculates sentiment metrics (precision, recall, F1, accuracy) for each review,\n",
    "            considering all predicted sentiments against all ground truth sentiments.\n",
    "        4.  Stores the per-review metrics in a list.\n",
    "        5.  Averages the per-review metrics to obtain the final, overall metrics.\n",
    "\n",
    "    Note:\n",
    "        -   Aspect matching is case-insensitive.\n",
    "        -   Sentiment evaluation is performed considering all predicted sentiments versus all ground truth sentiments.\n",
    "        -   This function is designed to provide a more granular evaluation by treating each review\n",
    "            as an independent unit and then averaging the performance.\n",
    "        -   **Handling Different Number of Aspects:**\n",
    "            -   **Aspect Evaluation:** The use of sets (`predicted_aspects`, `actual_aspects`) and set operations (intersection)\n",
    "                                ensures fair comparison regardless of differing numbers of aspects. Precision and recall are calculated\n",
    "                                to penalize extra or missing aspects, respectively.\n",
    "            -   **Example:** If a prediction review has 2 aspects and the ground truth has 3, the aspect recall will be affected,\n",
    "                                Similarly, if a prediction has extra aspects, the aspect precision will be affected\n",
    "    \"\"\"\n",
    "    review_metrics = []  # List to store metrics for each review\n",
    "\n",
    "    # Iterate through each review's predictions and ground truth\n",
    "    for predictions, ground_truth in zip(predictions_list, ground_truth_list):\n",
    "        # Aspect Evaluation\n",
    "        predicted_aspects = {p[0].lower() for p in predictions}  # Set of predicted aspects (lowercase)\n",
    "        actual_aspects = {gt[0].lower() for gt in ground_truth}    # Set of actual aspects (lowercase)\n",
    "\n",
    "        # Calculate aspect metrics for the current review\n",
    "        aspect_correct = len(predicted_aspects.intersection(actual_aspects))  # Number of correctly predicted aspects, correctly identifies the aspects that are common to both the prediction and the ground truth\n",
    "        aspect_predicted = len(predicted_aspects)  # Total number of predicted aspects\n",
    "        aspect_actual = len(actual_aspects)      # Total number of actual aspects\n",
    "\n",
    "        # Calculate precision, recall, F1, and accuracy for aspects\n",
    "        aspect_precision = aspect_correct / aspect_predicted if aspect_predicted > 0 else 0  # penalizes the model for predicting extra aspects that are not in the ground truth.\n",
    "        aspect_recall = aspect_correct / aspect_actual if aspect_actual > 0 else 0  # penalizes the model for missing aspects that are present in the ground truth.\n",
    "        aspect_f1 = 2 * (aspect_precision * aspect_recall) / (aspect_precision + aspect_recall) if (aspect_precision + aspect_recall) > 0 else 0\n",
    "        aspect_accuracy = aspect_correct / aspect_actual if aspect_actual > 0 else 0\n",
    "\n",
    "        # Sentiment Evaluation (considering all sentiments)\n",
    "        sentiment_correct = 0\n",
    "        sentiment_predicted = len(predictions)  # Total number of predicted sentiments.\n",
    "        sentiment_actual = len(ground_truth)    # Total number of actual sentiments.\n",
    "\n",
    "        # Calculate correctly predicted sentiments\n",
    "        for pred in predictions:\n",
    "            for truth in ground_truth:\n",
    "                if pred[0].lower() == truth[0].lower() and pred[1] == truth[1]:\n",
    "                    sentiment_correct += 1\n",
    "\n",
    "        # Calculate precision, recall, F1, and accuracy for sentiments\n",
    "        sentiment_precision = sentiment_correct / sentiment_predicted if sentiment_predicted > 0 else 0\n",
    "        sentiment_recall = sentiment_correct / sentiment_actual if sentiment_actual > 0 else 0\n",
    "        sentiment_f1 = 2 * (sentiment_precision * sentiment_recall) / (sentiment_precision + sentiment_recall) if (sentiment_precision + sentiment_recall) > 0 else 0\n",
    "        sentiment_accuracy = sentiment_correct / sentiment_actual if sentiment_actual > 0 else 0\n",
    "\n",
    "        # Store metrics for the current review\n",
    "        review_metrics.append({\n",
    "            \"Aspect\": {\n",
    "                \"Precision\": aspect_precision,\n",
    "                \"Recall\": aspect_recall,\n",
    "                \"F1\": aspect_f1,\n",
    "                \"Accuracy\": aspect_accuracy,\n",
    "            },\n",
    "            \"Sentiment\": {\n",
    "                \"Precision\": sentiment_precision,\n",
    "                \"Recall\": sentiment_recall,\n",
    "                \"F1\": sentiment_f1,\n",
    "                \"Accuracy\": sentiment_accuracy,\n",
    "            },\n",
    "        })\n",
    "\n",
    "    # Average metrics\n",
    "    avg_metrics = {\n",
    "        \"Aspect\": {\"Precision\": 0, \"Recall\": 0, \"F1\": 0, \"Accuracy\": 0},\n",
    "        \"Sentiment\": {\"Precision\": 0, \"Recall\": 0, \"F1\": 0, \"Accuracy\": 0},\n",
    "    }\n",
    "\n",
    "    num_reviews = len(review_metrics)  # Number of reviews\n",
    "    if num_reviews == 0:\n",
    "        return \"No review metrics provided.\"\n",
    "\n",
    "    # Sum up metrics from all reviews\n",
    "    for review in review_metrics:\n",
    "        for metric_type in [\"Aspect\", \"Sentiment\"]:\n",
    "            for metric in [\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]:\n",
    "                avg_metrics[metric_type][metric] += review[metric_type][metric]\n",
    "\n",
    "    # Calculate average metrics\n",
    "    for metric_type in [\"Aspect\", \"Sentiment\"]:\n",
    "        for metric in [\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]:\n",
    "            avg_metrics[metric_type][metric] /= num_reviews\n",
    "\n",
    "    # Format the output for better readability\n",
    "    output = \"\"\n",
    "    for metric_type in [\"Aspect\", \"Sentiment\"]:\n",
    "        output += f\"{metric_type}: (\\n\"\n",
    "        for metric, value in avg_metrics[metric_type].items():\n",
    "            output += f\"   '{metric}': {value:.4f}\\n\"\n",
    "        output += \")\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads data from a JSONL file (one JSON object per line)\n",
    "    or a single JSON array file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0) # Go back to the beginning of the file\n",
    "\n",
    "        # Assume JSON format\n",
    "        if first_char == '[': # It's likely a single JSON array\n",
    "            try:\n",
    "                data = json.load(f) # Load the entire file as one JSON structure\n",
    "                if not isinstance(data, list):\n",
    "                    print(f\"Warning: File {file_path} started with '[' but is not a JSON array. Skipping.\")\n",
    "                    data = []\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON array from {file_path}: {e}\")\n",
    "                data = []\n",
    "        else: # Assume JSONL format\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    if isinstance(item, dict):\n",
    "                        data.append(item)\n",
    "                    else:\n",
    "                        print(f\"Warning: Skipping non-dictionary entry in {file_path}: {line}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Warning: Skipping invalid JSON line in {file_path}: {line} - Error: {e}\")\n",
    "    return data\n",
    "\n",
    "def convert_data_for_evaluation(data, name='labels'):\n",
    "    \"\"\"\n",
    "    Converts data to a format suitable for aspect and sentiment evaluation.\n",
    "    Handles cases where 'category' might be missing and fills it with None.\n",
    "\n",
    "    Args:\n",
    "        data: A list of dictionaries, where each dictionary contains a 'labels' key (or other specified key).\n",
    "        name: The key to access the list of predictions within each dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains tuples of \n",
    "        (aspect, polarity, opinion, category).\n",
    "    \"\"\"\n",
    "    aspect_sentiment_pairs = []\n",
    "    for item in data:\n",
    "        predictions = item.get(name, [])\n",
    "        if not isinstance(predictions, list):\n",
    "            print(f\"Warning: Skipping item with invalid predictions: {item}\")\n",
    "            continue\n",
    "\n",
    "        converted_predictions = []\n",
    "        for pred in predictions:\n",
    "            if not isinstance(pred, dict):\n",
    "                print(f\"Warning: Skipping non-dictionary prediction: {pred}\")\n",
    "                continue\n",
    "\n",
    "            aspect = pred.get('aspect', None)\n",
    "            polarity = pred.get('polarity', None)\n",
    "            opinion = pred.get('opinion', None)\n",
    "            category = pred.get('category', None)\n",
    "\n",
    "            # Skip predictions with missing aspect or polarity or opinion\n",
    "            if aspect is not None and polarity is not None and opinion is not None:\n",
    "                converted_predictions.append((aspect, polarity, opinion, category))\n",
    "            else:\n",
    "                print(f\"Warning: Skipping prediction with missing data: {pred}\")\n",
    "\n",
    "        aspect_sentiment_pairs.append(converted_predictions)\n",
    "    return aspect_sentiment_pairs\n",
    "\n",
    "# Define a function to handle the evaluation for a single model\n",
    "def run_model_evaluation(model_name, prediction_file_path, ground_truth_file_path):\n",
    "    \"\"\"\n",
    "    Reads prediction and ground truth data, converts it, and prints evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model being evaluated (for printing).\n",
    "        prediction_file_path (str): Path to the prediction JSONL/JSON file.\n",
    "        ground_truth_file_path (str): Path to the ground truth JSONL file.\n",
    "    \"\"\"\n",
    "    print(f\"##### Testing Metrics - {model_name} #####\")\n",
    "\n",
    "    # Read predictions\n",
    "    predictions_data = read_jsonl(prediction_file_path) # Use read_json_flexible if your JSON files are arrays\n",
    "    model_predictions = convert_data_for_evaluation(predictions_data)\n",
    "\n",
    "    # Read ground truth (only need to read once if it's the same for all models)\n",
    "    # However, keeping it inside the function makes it self-contained for each call\n",
    "    ground_truth_data = read_jsonl(ground_truth_file_path) # Use read_json_flexible if your JSON files are arrays\n",
    "    actual_ground_truth = convert_data_for_evaluation(ground_truth_data, name='labels')\n",
    "\n",
    "    # Ensure both lists have the same number of reviews before evaluating\n",
    "    if len(model_predictions) != len(actual_ground_truth):\n",
    "        print(f\"Warning: Number of predictions ({len(model_predictions)}) does not match \"\n",
    "              f\"number of ground truth reviews ({len(actual_ground_truth)}) for {model_name}. \"\n",
    "              \"Evaluation might be skewed or incomplete.\")\n",
    "        # You might choose to truncate, pad, or skip evaluation here based on your policy.\n",
    "        # For simplicity, we'll proceed, but be aware of this potential issue.\n",
    "\n",
    "    results = evaluate_aspect_and_sentiment(model_predictions, actual_ground_truth)\n",
    "    print(results)\n",
    "    print(\"-\" * 50) # Separator for readability\n",
    "\n",
    "# Paths to ground truth (remains constant)\n",
    "GROUND_TRUTH_FILE = \"../datasets/laptop_quad_test.tsv.jsonl\"\n",
    "\n",
    "# Dictionary of models to evaluate\n",
    "models_to_evaluate = {\n",
    "    \"Previous Best - DeepSeek-R1-Distill-Qwen\": \"../datasets/clean_full_results2.jsonl\",\n",
    "    # \"DeepSeek Base\": \"../datasets/DeepSeek-7B-Base_predictions.json\",\n",
    "    # \"Qwen Base\": \"../datasets/Qwen3-8B-Base_predictions.json\",\n",
    "    # \"DeepSeek-R1-Distill-Qwen\": \"../datasets/DeepSeek-R1-Distill-Qwen-7B_predictions.json\",\n",
    "    # Add more models here as needed:\n",
    "    # \"Another Model\": \"../datasets/another_model_predictions.jsonl\",\n",
    "}\n",
    "\n",
    "# Run evaluation for each model\n",
    "for model_name, prediction_path in models_to_evaluate.items():\n",
    "    run_model_evaluation(model_name, prediction_path, GROUND_TRUTH_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy Metrics:\n",
      "{'Aspect': 0.7163398692810458, 'Sentiment': 0.7006740196078431}\n",
      "\n",
      "Checking for invalid accuracy values (outside [0.0, 1.0]):\n",
      "All accuracy values are within valid range [0.0, 1.0]\n",
      "\n",
      "Enhanced Evaluation Results:\n",
      "Accuracy metrics:\n",
      "  Aspect (intersection/max): 0.7163\n",
      "  Sentiment (on matched aspects): 0.7007\n",
      "\n",
      "Standard metrics for comparison:\n",
      "  Aspect precision: 0.7493\n",
      "  Aspect recall: 0.7343\n",
      "  Aspect F1: 0.7329\n",
      "\n",
      "Detailed Error Analysis for first review:\n",
      "  False positive aspects: ['unit cost']\n",
      "  False negative aspects: ['unit']\n",
      "  Correct aspect wrong sentiment: []\n"
     ]
    }
   ],
   "source": [
    "def evaluate_aspect_and_sentiment(predictions_list, ground_truth_list, review_texts=None):\n",
    "    \"\"\"\n",
    "    Evaluates aspect and sentiment analysis results on a per-review basis.\n",
    "    - Aspect: Uses intersection/max approach to calculate accuracy\n",
    "    - Sentiment: Uses two-stage approach - only evaluates sentiment on matched aspects\n",
    "\n",
    "    Args:\n",
    "        predictions_list: A list of lists, where each inner list contains tuples of (aspect, polarity, opinion, category).\n",
    "                          Each inner list represents the predictions for a single review.\n",
    "        ground_truth_list: A list of lists, where each inner list contains tuples of (aspect, polarity, opinion, category).\n",
    "                          Each inner list represents the ground truth for a single review.\n",
    "        review_texts: (Optional) A list of review texts corresponding to each review.\n",
    "                      The length of each text will be used.\n",
    "\n",
    "    Returns:\n",
    "        final_accuracy: A dictionary with the averaged aspect and sentiment accuracy.\n",
    "                        Format: {'Aspect': <avg_aspect_accuracy>, 'Sentiment': <avg_sentiment_accuracy>}\n",
    "        per_review_output: A list of arrays for each review in the format:\n",
    "                           [review_text_length, aspect_accuracy, sentiment_accuracy]\n",
    "    \"\"\"\n",
    "    per_review_output = []  # Store per-review metrics\n",
    "    sum_aspect_accuracy = 0\n",
    "    sum_sentiment_accuracy = 0\n",
    "    num_reviews = 0\n",
    "\n",
    "    # Process each review's predictions and ground truth\n",
    "    for idx, (predictions, ground_truth) in enumerate(zip(predictions_list, ground_truth_list)):\n",
    "        # Aspect Evaluation - using sets to find unique aspects\n",
    "        predicted_aspects = {p[0].lower() for p in predictions}\n",
    "        actual_aspects = {gt[0].lower() for gt in ground_truth}\n",
    "\n",
    "        # Intersection = correctly predicted aspects\n",
    "        matched_aspects = predicted_aspects.intersection(actual_aspects)\n",
    "        aspect_correct = len(matched_aspects)\n",
    "        \n",
    "        # Calculate aspect accuracy using intersection/max approach\n",
    "        if len(actual_aspects) == 0 and len(predicted_aspects) == 0:\n",
    "            aspect_accuracy = 1.0  # Perfect accuracy when no aspects exist and none predicted\n",
    "        elif len(actual_aspects) == 0 or len(predicted_aspects) == 0:\n",
    "            aspect_accuracy = 0.0  # Zero accuracy when aspects exist but none predicted (or vice versa)\n",
    "        else:\n",
    "            # Formula: intersection divided by max of either set\n",
    "            aspect_accuracy = aspect_correct / max(len(actual_aspects), len(predicted_aspects))\n",
    "\n",
    "        # TWO-STAGE SENTIMENT EVALUATION\n",
    "        # 1. First identify matched aspects (already done above)\n",
    "        # 2. For matched aspects, calculate sentiment accuracy\n",
    "        \n",
    "        # Create mappings from aspect to sentiment\n",
    "        true_aspect_to_sentiment = {}\n",
    "        pred_aspect_to_sentiment = {}\n",
    "        \n",
    "        # Build ground truth mapping\n",
    "        for gt in ground_truth:\n",
    "            aspect = gt[0].lower()\n",
    "            # If multiple sentiments exist for the same aspect, keep the first one\n",
    "            if aspect not in true_aspect_to_sentiment:\n",
    "                true_aspect_to_sentiment[aspect] = gt[1]\n",
    "        \n",
    "        # Build prediction mapping\n",
    "        for pred in predictions:\n",
    "            aspect = pred[0].lower()\n",
    "            # If multiple sentiments exist for the same aspect, keep the first one\n",
    "            if aspect not in pred_aspect_to_sentiment:\n",
    "                pred_aspect_to_sentiment[aspect] = pred[1]\n",
    "        \n",
    "        # Count sentiment matches ONLY for matched aspects\n",
    "        sentiment_correct = 0\n",
    "        for aspect in matched_aspects:\n",
    "            if aspect in pred_aspect_to_sentiment and aspect in true_aspect_to_sentiment:\n",
    "                if pred_aspect_to_sentiment[aspect] == true_aspect_to_sentiment[aspect]:\n",
    "                    sentiment_correct += 1\n",
    "        \n",
    "        # Calculate sentiment accuracy only on matched aspects\n",
    "        if len(matched_aspects) == 0:\n",
    "            sentiment_accuracy = 0.0  # No matched aspects to evaluate sentiment on\n",
    "        else:\n",
    "            sentiment_accuracy = sentiment_correct / len(matched_aspects)\n",
    "\n",
    "        # Determine review text length (if provided, else 0)\n",
    "        if review_texts is not None and idx < len(review_texts):\n",
    "            text_length = len(review_texts[idx])\n",
    "        else:\n",
    "            text_length = 0\n",
    "\n",
    "        per_review_output.append([text_length, aspect_accuracy, sentiment_accuracy])\n",
    "        sum_aspect_accuracy += aspect_accuracy\n",
    "        sum_sentiment_accuracy += sentiment_accuracy\n",
    "        num_reviews += 1\n",
    "\n",
    "    # Final averaged accuracy metrics\n",
    "    if num_reviews > 0:\n",
    "        final_accuracy = {\n",
    "            \"Aspect\": sum_aspect_accuracy / num_reviews,\n",
    "            \"Sentiment\": sum_sentiment_accuracy / num_reviews,\n",
    "        }\n",
    "    else:\n",
    "        final_accuracy = {\"Aspect\": 0, \"Sentiment\": 0}\n",
    "\n",
    "    return final_accuracy, per_review_output\n",
    "\n",
    "\n",
    "def enhanced_evaluate_aspect_and_sentiment(predictions_list, ground_truth_list, review_texts=None, \n",
    "                                           ignore_case=True, include_standard_metrics=False):\n",
    "    \"\"\"\n",
    "    Enhanced evaluation with two-stage sentiment approach and detailed error analysis.\n",
    "    \n",
    "    Args:\n",
    "        predictions_list: List of prediction tuples per review\n",
    "        ground_truth_list: List of ground truth tuples per review\n",
    "        review_texts: Optional list of review texts\n",
    "        ignore_case: Whether to perform case-insensitive comparison of aspects\n",
    "        include_standard_metrics: Whether to include precision, recall, and F1 metrics\n",
    "        \n",
    "    Returns:\n",
    "        final_metrics: Dict with accuracy metrics\n",
    "        per_review_output: List of detailed metrics for each review\n",
    "    \"\"\"\n",
    "    per_review_output = []\n",
    "    sum_aspect_accuracy = 0\n",
    "    sum_sentiment_accuracy = 0\n",
    "    \n",
    "    # If including standard metrics\n",
    "    if include_standard_metrics:\n",
    "        sum_aspect_precision = 0\n",
    "        sum_aspect_recall = 0\n",
    "        sum_aspect_f1 = 0\n",
    "        sum_sentiment_precision = 0\n",
    "        sum_sentiment_recall = 0\n",
    "        sum_sentiment_f1 = 0\n",
    "    \n",
    "    num_reviews = 0\n",
    "    \n",
    "    # Process each review\n",
    "    for idx, (predictions, ground_truth) in enumerate(zip(predictions_list, ground_truth_list)):\n",
    "        review_metrics = {}\n",
    "        \n",
    "        # Get review text length if available\n",
    "        if review_texts is not None and idx < len(review_texts):\n",
    "            review_metrics['text_length'] = len(review_texts[idx])\n",
    "        else:\n",
    "            review_metrics['text_length'] = 0\n",
    "        \n",
    "        # Convert aspects based on case sensitivity setting\n",
    "        if ignore_case:\n",
    "            # Case-insensitive comparison (convert all to lowercase)\n",
    "            pred_aspects = [p[0].lower() for p in predictions]\n",
    "            true_aspects = [gt[0].lower() for gt in ground_truth]\n",
    "        else:\n",
    "            # Case-sensitive comparison\n",
    "            pred_aspects = [p[0] for p in predictions]\n",
    "            true_aspects = [gt[0] for gt in ground_truth]\n",
    "        \n",
    "        # Get unique aspects\n",
    "        unique_pred_aspects = set(pred_aspects)\n",
    "        unique_true_aspects = set(true_aspects)\n",
    "        \n",
    "        # Get matched aspects (intersection)\n",
    "        matched_aspects = unique_pred_aspects.intersection(unique_true_aspects)\n",
    "        \n",
    "        # Aspect accuracy using intersection/max formula\n",
    "        if len(unique_true_aspects) == 0 and len(unique_pred_aspects) == 0:\n",
    "            aspect_accuracy = 1.0\n",
    "        elif len(unique_true_aspects) == 0 or len(unique_pred_aspects) == 0:\n",
    "            aspect_accuracy = 0.0\n",
    "        else:\n",
    "            aspect_accuracy = len(matched_aspects) / max(len(unique_pred_aspects), len(unique_true_aspects))\n",
    "        \n",
    "        # Create mappings from aspect to sentiment\n",
    "        true_aspect_to_sentiment = {}\n",
    "        pred_aspect_to_sentiment = {}\n",
    "        \n",
    "        # Build mappings (keeping first occurrence of each aspect)\n",
    "        for i, aspect in enumerate(true_aspects):\n",
    "            if aspect not in true_aspect_to_sentiment:\n",
    "                true_aspect_to_sentiment[aspect] = ground_truth[i][1]\n",
    "                \n",
    "        for i, aspect in enumerate(pred_aspects):\n",
    "            if aspect not in pred_aspect_to_sentiment:\n",
    "                pred_aspect_to_sentiment[aspect] = predictions[i][1]\n",
    "        \n",
    "        # Two-stage sentiment evaluation\n",
    "        sentiment_correct = 0\n",
    "        for aspect in matched_aspects:\n",
    "            if pred_aspect_to_sentiment.get(aspect) == true_aspect_to_sentiment.get(aspect):\n",
    "                sentiment_correct += 1\n",
    "                \n",
    "        # Calculate sentiment accuracy on matched aspects\n",
    "        if len(matched_aspects) == 0:\n",
    "            sentiment_accuracy = 0.0\n",
    "        else:\n",
    "            sentiment_accuracy = sentiment_correct / len(matched_aspects)\n",
    "        \n",
    "        # Store primary metrics\n",
    "        review_metrics['aspect_accuracy'] = aspect_accuracy\n",
    "        review_metrics['sentiment_accuracy'] = sentiment_accuracy\n",
    "        \n",
    "        # Store detailed metrics\n",
    "        review_metrics['details'] = {\n",
    "            'matched_aspects': len(matched_aspects),\n",
    "            'total_pred_aspects': len(unique_pred_aspects),\n",
    "            'total_true_aspects': len(unique_true_aspects),\n",
    "            'sentiment_correct': sentiment_correct\n",
    "        }\n",
    "        \n",
    "        # Calculate standard metrics if requested\n",
    "        if include_standard_metrics:\n",
    "            # Precision = correct / predicted\n",
    "            aspect_precision = len(matched_aspects) / len(unique_pred_aspects) if len(unique_pred_aspects) > 0 else 0\n",
    "            # Recall = correct / actual\n",
    "            aspect_recall = len(matched_aspects) / len(unique_true_aspects) if len(unique_true_aspects) > 0 else 0\n",
    "            # F1 = 2 * precision * recall / (precision + recall)\n",
    "            aspect_f1 = 0\n",
    "            if aspect_precision + aspect_recall > 0:\n",
    "                aspect_f1 = 2 * aspect_precision * aspect_recall / (aspect_precision + aspect_recall)\n",
    "            \n",
    "            # Store standard metrics\n",
    "            review_metrics['aspect_standard'] = {\n",
    "                'precision': aspect_precision,\n",
    "                'recall': aspect_recall,\n",
    "                'f1': aspect_f1\n",
    "            }\n",
    "            \n",
    "            # Update sums for standard metrics\n",
    "            sum_aspect_precision += aspect_precision\n",
    "            sum_aspect_recall += aspect_recall\n",
    "            sum_aspect_f1 += aspect_f1\n",
    "        \n",
    "        # Error analysis\n",
    "        false_positive_aspects = unique_pred_aspects - unique_true_aspects\n",
    "        false_negative_aspects = unique_true_aspects - unique_pred_aspects\n",
    "        \n",
    "        # Find aspects with correct identification but wrong sentiment\n",
    "        correct_aspect_wrong_sentiment = []\n",
    "        for aspect in matched_aspects:\n",
    "            if pred_aspect_to_sentiment.get(aspect) != true_aspect_to_sentiment.get(aspect):\n",
    "                correct_aspect_wrong_sentiment.append({\n",
    "                    'aspect': aspect,\n",
    "                    'predicted_sentiment': pred_aspect_to_sentiment.get(aspect),\n",
    "                    'true_sentiment': true_aspect_to_sentiment.get(aspect)\n",
    "                })\n",
    "        \n",
    "        review_metrics['error_analysis'] = {\n",
    "            'false_positive_aspects': list(false_positive_aspects),\n",
    "            'false_negative_aspects': list(false_negative_aspects),\n",
    "            'correct_aspect_wrong_sentiment': correct_aspect_wrong_sentiment\n",
    "        }\n",
    "        \n",
    "        per_review_output.append(review_metrics)\n",
    "        sum_aspect_accuracy += aspect_accuracy\n",
    "        sum_sentiment_accuracy += sentiment_accuracy\n",
    "        num_reviews += 1\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    final_metrics = {\n",
    "        'Aspect': sum_aspect_accuracy / num_reviews if num_reviews > 0 else 0,\n",
    "        'Sentiment': sum_sentiment_accuracy / num_reviews if num_reviews > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Add standard metrics if requested\n",
    "    if include_standard_metrics and num_reviews > 0:\n",
    "        final_metrics['Aspect_Standard'] = {\n",
    "            'precision': sum_aspect_precision / num_reviews,\n",
    "            'recall': sum_aspect_recall / num_reviews,\n",
    "            'f1': sum_aspect_f1 / num_reviews\n",
    "        }\n",
    "    \n",
    "    return final_metrics, per_review_output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # First use the basic evaluation function (direct replacement for original code)\n",
    "    final_accuracy, per_review_array = evaluate_aspect_and_sentiment(\n",
    "        predictions_list[0], ground_truth_list[0], review_texts\n",
    "    )\n",
    "    \n",
    "    print(\"Final Accuracy Metrics:\")\n",
    "    print(final_accuracy)\n",
    "    print(\"\\nChecking for invalid accuracy values (outside [0.0, 1.0]):\")\n",
    "    invalid_found = False\n",
    "    for i in range(len(per_review_array)):\n",
    "        aspect_accuracy = per_review_array[i][1]\n",
    "        sentiment_accuracy = per_review_array[i][2]\n",
    "        \n",
    "        if aspect_accuracy > 1.0 or aspect_accuracy < 0.0 or sentiment_accuracy > 1.0 or sentiment_accuracy < 0.0:\n",
    "            print(f\"Review {i+1}: {per_review_array[i]} - Invalid accuracy detected!\")\n",
    "            invalid_found = True\n",
    "\n",
    "    if not invalid_found:\n",
    "        print(\"All accuracy values are within valid range [0.0, 1.0]\")\n",
    "    \n",
    "    # Then use the enhanced version with more detailed output\n",
    "    final_metrics, detailed_metrics = enhanced_evaluate_aspect_and_sentiment(\n",
    "        predictions_list[0], ground_truth_list[0], review_texts,\n",
    "        ignore_case=True, include_standard_metrics=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEnhanced Evaluation Results:\")\n",
    "    print(\"Accuracy metrics:\")\n",
    "    print(f\"  Aspect (intersection/max): {final_metrics['Aspect']:.4f}\")\n",
    "    print(f\"  Sentiment (on matched aspects): {final_metrics['Sentiment']:.4f}\")\n",
    "    \n",
    "    if 'Aspect_Standard' in final_metrics:\n",
    "        print(\"\\nStandard metrics for comparison:\")\n",
    "        print(f\"  Aspect precision: {final_metrics['Aspect_Standard']['precision']:.4f}\")\n",
    "        print(f\"  Aspect recall: {final_metrics['Aspect_Standard']['recall']:.4f}\")\n",
    "        print(f\"  Aspect F1: {final_metrics['Aspect_Standard']['f1']:.4f}\")\n",
    "    \n",
    "    # Show detailed error analysis for one review\n",
    "    if len(detailed_metrics) > 0:\n",
    "        print(\"\\nDetailed Error Analysis for first review:\")\n",
    "        errors = detailed_metrics[0]['error_analysis']\n",
    "        print(f\"  False positive aspects: {errors['false_positive_aspects']}\")\n",
    "        print(f\"  False negative aspects: {errors['false_negative_aspects']}\")\n",
    "        print(f\"  Correct aspect wrong sentiment: {errors['correct_aspect_wrong_sentiment']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics successfully written to review_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def write_metrics_to_csv(per_review_array, output_filepath):\n",
    "    \"\"\"\n",
    "    Writes review metrics (text length, aspect accuracy, sentiment accuracy) to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        per_review_array: List of arrays, where each array contains \n",
    "                          [review_text_length, aspect_accuracy, sentiment_accuracy]\n",
    "        output_filepath: Path where the CSV file will be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_filepath, 'w', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            \n",
    "            # Write header\n",
    "            csvwriter.writerow(['Text_Length', 'Aspect_Accuracy', 'Sentiment_Accuracy'])\n",
    "            \n",
    "            # Write data for each review\n",
    "            for metrics in per_review_array:\n",
    "                text_length = metrics[0]\n",
    "                aspect_accuracy = metrics[1]\n",
    "                sentiment_accuracy = metrics[2]\n",
    "                \n",
    "                csvwriter.writerow([text_length, aspect_accuracy, sentiment_accuracy])\n",
    "                \n",
    "        print(f\"Metrics successfully written to {output_filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to CSV: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # After running evaluation\n",
    "    final_accuracy, per_review_array = evaluate_aspect_and_sentiment(\n",
    "        predictions_list[0], ground_truth_list[0], review_texts\n",
    "    )\n",
    "    \n",
    "    # Write metrics to CSV\n",
    "    write_metrics_to_csv(per_review_array, \"review_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
