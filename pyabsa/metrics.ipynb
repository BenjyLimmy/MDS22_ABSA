{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "{\n",
      "    \"aspect_sentiment\": {\n",
      "        \"precision\": 0.6485097636176773,\n",
      "        \"recall\": 0.6272365805168986,\n",
      "        \"f1\": 0.6376958059626074\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads test data and results data from two JSONL files.\n",
    "    Skips lines that cannot be parsed as valid JSON.\n",
    "    \"\"\"\n",
    "    test_file_path = \"../datasets/laptop_quad_test.tsv.jsonl\"\n",
    "    results_file_path = \"../datasets/clean_full_results2.jsonl\"\n",
    "    \n",
    "    test_data = []\n",
    "    results_data = []\n",
    "    \n",
    "    # Load test data\n",
    "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Skip empty lines\n",
    "                continue\n",
    "            try:\n",
    "                data_item = json.loads(line)\n",
    "                test_data.append(data_item)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that aren't valid JSON\n",
    "                continue\n",
    "            \n",
    "    # Load results data\n",
    "    with open(results_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Skip empty lines\n",
    "                continue\n",
    "            try:\n",
    "                data_item = json.loads(line)\n",
    "                results_data.append(data_item)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that aren't valid JSON\n",
    "                continue\n",
    "            \n",
    "    return test_data, results_data\n",
    "\n",
    "def evaluate_aspect_and_sentiment(test_data, results_data):\n",
    "    \"\"\"\n",
    "    Evaluates aspect-sentiment pairs for precision, recall, and F1 score.\n",
    "    Skips any entries that are not dictionaries or do not have a valid 'labels' list.\n",
    "    \n",
    "    Debug statements print the test_label_set and result_label_set for every comparison.\n",
    "    \"\"\"\n",
    "    total_test_labels = 0\n",
    "    total_result_labels = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # Loop over both datasets in parallel\n",
    "    for test_item, result_item in zip(test_data, results_data):\n",
    "        # Ensure both items are dictionaries\n",
    "        if not isinstance(test_item, dict) or not isinstance(result_item, dict):\n",
    "            continue\n",
    "        \n",
    "        # Extract 'labels' as lists (skip if not a list)\n",
    "        test_labels = test_item.get(\"labels\", [])\n",
    "        result_labels = result_item.get(\"labels\", [])\n",
    "        if not isinstance(test_labels, list) or not isinstance(result_labels, list):\n",
    "            continue\n",
    "        \n",
    "        # Build sets of (aspect, polarity) from test data\n",
    "        test_label_set = set()\n",
    "        for label in test_labels:\n",
    "            if isinstance(label, dict):\n",
    "                aspect = label.get(\"aspect\")\n",
    "                polarity = label.get(\"polarity\")\n",
    "                test_label_set.add((aspect, polarity))\n",
    "        \n",
    "        # Build sets of (aspect, polarity) from results data\n",
    "        result_label_set = set()\n",
    "        for label in result_labels:\n",
    "            if isinstance(label, dict):\n",
    "                aspect = label.get(\"aspect\")\n",
    "                polarity = label.get(\"polarity\")\n",
    "                result_label_set.add((aspect, polarity))\n",
    "\n",
    "        # Tally up counts\n",
    "        total_test_labels += len(test_label_set)\n",
    "        total_result_labels += len(result_label_set)\n",
    "        total_correct += len(test_label_set.intersection(result_label_set))\n",
    "    \n",
    "    # Compute precision, recall, F1\n",
    "    precision = total_correct / total_result_labels if total_result_labels else 0.0\n",
    "    recall = total_correct / total_test_labels if total_test_labels else 0.0\n",
    "    f1 = (\n",
    "        (2 * precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"aspect_sentiment\": {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def run_evaluation():\n",
    "    # Load data\n",
    "    test_data, results_data = load_data()\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_aspect_and_sentiment(test_data, results_data)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(json.dumps(metrics, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Testing Metrics#####\n",
      "Aspect: (\n",
      "   'Precision': 0.7493\n",
      "   'Recall': 0.7343\n",
      "   'F1': 0.7329\n",
      "   'Accuracy': 0.7343\n",
      ")\n",
      "Sentiment: (\n",
      "   'Precision': 0.7860\n",
      "   'Recall': 0.8280\n",
      "   'F1': 0.7767\n",
      "   'Accuracy': 0.8280\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment evaluation is performed considering all predicted sentiments regardless of the wrong/correct aspects \n",
    "\n",
    "def evaluate_aspect_and_sentiment(predictions_list, ground_truth_list):\n",
    "    \"\"\"\n",
    "    Evaluates aspect and sentiment analysis results on a per-review basis, then averages the metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions_list: A list of lists, where each inner list contains tuples of (aspect, sentiment, ...).\n",
    "                            Each inner list represents the predictions for a single review.\n",
    "        ground_truth_list: A list of lists, where each inner list contains tuples of (aspect, sentiment, ...).\n",
    "                            Each inner list represents the ground truth for a single review.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the averaged aspect and sentiment evaluation metrics.\n",
    "\n",
    "    Functionality:\n",
    "        1.  Iterates through each review's predictions and ground truth.\n",
    "        2.  Calculates aspect metrics (precision, recall, F1, accuracy) for each review.\n",
    "        3.  Calculates sentiment metrics (precision, recall, F1, accuracy) for each review,\n",
    "            considering all predicted sentiments against all ground truth sentiments.\n",
    "        4.  Stores the per-review metrics in a list.\n",
    "        5.  Averages the per-review metrics to obtain the final, overall metrics.\n",
    "\n",
    "    Note:\n",
    "        -   Aspect matching is case-insensitive.\n",
    "        -   Sentiment evaluation is performed considering all predicted sentiments versus all ground truth sentiments.\n",
    "        -   This function is designed to provide a more granular evaluation by treating each review\n",
    "            as an independent unit and then averaging the performance.\n",
    "        -   **Handling Different Number of Aspects:**\n",
    "            -   **Aspect Evaluation:** The use of sets (`predicted_aspects`, `actual_aspects`) and set operations (intersection)\n",
    "                                ensures fair comparison regardless of differing numbers of aspects. Precision and recall are calculated\n",
    "                                to penalize extra or missing aspects, respectively.\n",
    "            -   **Example:** If a prediction review has 2 aspects and the ground truth has 3, the aspect recall will be affected,\n",
    "                                Similarly, if a prediction has extra aspects, the aspect precision will be affected\n",
    "    \"\"\"\n",
    "    review_metrics = []  # List to store metrics for each review\n",
    "\n",
    "    # Iterate through each review's predictions and ground truth\n",
    "    for predictions, ground_truth in zip(predictions_list, ground_truth_list):\n",
    "        # Aspect Evaluation\n",
    "        predicted_aspects = {p[0].lower() for p in predictions}  # Set of predicted aspects (lowercase)\n",
    "        actual_aspects = {gt[0].lower() for gt in ground_truth}    # Set of actual aspects (lowercase)\n",
    "\n",
    "        # Calculate aspect metrics for the current review\n",
    "        aspect_correct = len(predicted_aspects.intersection(actual_aspects))  # Number of correctly predicted aspects, correctly identifies the aspects that are common to both the prediction and the ground truth\n",
    "        aspect_predicted = len(predicted_aspects)  # Total number of predicted aspects\n",
    "        aspect_actual = len(actual_aspects)      # Total number of actual aspects\n",
    "\n",
    "        # Calculate precision, recall, F1, and accuracy for aspects\n",
    "        aspect_precision = aspect_correct / aspect_predicted if aspect_predicted > 0 else 0  # penalizes the model for predicting extra aspects that are not in the ground truth.\n",
    "        aspect_recall = aspect_correct / aspect_actual if aspect_actual > 0 else 0  # penalizes the model for missing aspects that are present in the ground truth.\n",
    "        aspect_f1 = 2 * (aspect_precision * aspect_recall) / (aspect_precision + aspect_recall) if (aspect_precision + aspect_recall) > 0 else 0\n",
    "        aspect_accuracy = aspect_correct / aspect_actual if aspect_actual > 0 else 0\n",
    "\n",
    "        # Sentiment Evaluation (considering all sentiments)\n",
    "        sentiment_correct = 0\n",
    "        sentiment_predicted = len(predictions)  # Total number of predicted sentiments.\n",
    "        sentiment_actual = len(ground_truth)    # Total number of actual sentiments.\n",
    "\n",
    "        # Calculate correctly predicted sentiments\n",
    "        for pred in predictions:\n",
    "            for truth in ground_truth:\n",
    "                if pred[0].lower() == truth[0].lower() and pred[1] == truth[1]:\n",
    "                    sentiment_correct += 1\n",
    "\n",
    "        # Calculate precision, recall, F1, and accuracy for sentiments\n",
    "        sentiment_precision = sentiment_correct / sentiment_predicted if sentiment_predicted > 0 else 0\n",
    "        sentiment_recall = sentiment_correct / sentiment_actual if sentiment_actual > 0 else 0\n",
    "        sentiment_f1 = 2 * (sentiment_precision * sentiment_recall) / (sentiment_precision + sentiment_recall) if (sentiment_precision + sentiment_recall) > 0 else 0\n",
    "        sentiment_accuracy = sentiment_correct / sentiment_actual if sentiment_actual > 0 else 0\n",
    "\n",
    "        # Store metrics for the current review\n",
    "        review_metrics.append({\n",
    "            \"Aspect\": {\n",
    "                \"Precision\": aspect_precision,\n",
    "                \"Recall\": aspect_recall,\n",
    "                \"F1\": aspect_f1,\n",
    "                \"Accuracy\": aspect_accuracy,\n",
    "            },\n",
    "            \"Sentiment\": {\n",
    "                \"Precision\": sentiment_precision,\n",
    "                \"Recall\": sentiment_recall,\n",
    "                \"F1\": sentiment_f1,\n",
    "                \"Accuracy\": sentiment_accuracy,\n",
    "            },\n",
    "        })\n",
    "\n",
    "    # Average metrics\n",
    "    avg_metrics = {\n",
    "        \"Aspect\": {\"Precision\": 0, \"Recall\": 0, \"F1\": 0, \"Accuracy\": 0},\n",
    "        \"Sentiment\": {\"Precision\": 0, \"Recall\": 0, \"F1\": 0, \"Accuracy\": 0},\n",
    "    }\n",
    "\n",
    "    num_reviews = len(review_metrics)  # Number of reviews\n",
    "    if num_reviews == 0:\n",
    "        return \"No review metrics provided.\"\n",
    "\n",
    "    # Sum up metrics from all reviews\n",
    "    for review in review_metrics:\n",
    "        for metric_type in [\"Aspect\", \"Sentiment\"]:\n",
    "            for metric in [\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]:\n",
    "                avg_metrics[metric_type][metric] += review[metric_type][metric]\n",
    "\n",
    "    # Calculate average metrics\n",
    "    for metric_type in [\"Aspect\", \"Sentiment\"]:\n",
    "        for metric in [\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]:\n",
    "            avg_metrics[metric_type][metric] /= num_reviews\n",
    "\n",
    "    # Format the output for better readability\n",
    "    output = \"\"\n",
    "    for metric_type in [\"Aspect\", \"Sentiment\"]:\n",
    "        output += f\"{metric_type}: (\\n\"\n",
    "        for metric, value in avg_metrics[metric_type].items():\n",
    "            output += f\"   '{metric}': {value:.4f}\\n\"\n",
    "        output += \")\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Function to read data from JSONL file while skipping errors\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"Reads data from a JSONL file, skipping invalid lines.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                if isinstance(item, dict):\n",
    "                    data.append(item)\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping non-dictionary entry in {file_path}: {line}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Skipping invalid JSON line in {file_path}: {line} - Error: {e}\")\n",
    "    return data\n",
    "\n",
    "def convert_data_for_evaluation(data, name='labels'):\n",
    "    \"\"\"\n",
    "    Converts data to a format suitable for aspect and sentiment evaluation.\n",
    "    Handles cases where 'category' might be missing and fills it with None.\n",
    "\n",
    "    Args:\n",
    "        data: A list of dictionaries, where each dictionary contains a 'labels' key (or other specified key).\n",
    "        name: The key to access the list of predictions within each dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains tuples of \n",
    "        (aspect, polarity, opinion, category).\n",
    "    \"\"\"\n",
    "    aspect_sentiment_pairs = []\n",
    "    for item in data:\n",
    "        predictions = item.get(name, [])\n",
    "        if not isinstance(predictions, list):\n",
    "            print(f\"Warning: Skipping item with invalid predictions: {item}\")\n",
    "            continue\n",
    "\n",
    "        converted_predictions = []\n",
    "        for pred in predictions:\n",
    "            if not isinstance(pred, dict):\n",
    "                print(f\"Warning: Skipping non-dictionary prediction: {pred}\")\n",
    "                continue\n",
    "\n",
    "            aspect = pred.get('aspect', None)\n",
    "            polarity = pred.get('polarity', None)\n",
    "            opinion = pred.get('opinion', None)\n",
    "            category = pred.get('category', None)\n",
    "\n",
    "            # Skip predictions with missing aspect or polarity or opinion\n",
    "            if aspect is not None and polarity is not None and opinion is not None:\n",
    "                converted_predictions.append((aspect, polarity, opinion, category))\n",
    "            else:\n",
    "                print(f\"Warning: Skipping prediction with missing data: {pred}\")\n",
    "\n",
    "        aspect_sentiment_pairs.append(converted_predictions)\n",
    "    return aspect_sentiment_pairs\n",
    "\n",
    "# Paths to output JSONL files containing predictions\n",
    "# output_files = [\"../datasets/clean_full_results.jsonl\"]\n",
    "output_files = [\"../datasets/clean_full_results2.jsonl\"]\n",
    "# output_files = [\"../datasets/deepseek_r1_results.jsonl\"]\n",
    "\n",
    "# Paths to ground truth JSONL files\n",
    "ground_truth_files = [\"../datasets/laptop_quad_test.tsv.jsonl\"]\n",
    "\n",
    "# Read predictions from output files\n",
    "predictions_list = []\n",
    "for output_file in output_files:\n",
    "    predictions_data = read_jsonl(output_file)\n",
    "    predictions_list.append(convert_data_for_evaluation(predictions_data))\n",
    "\n",
    "# Read ground truth from ground truth files\n",
    "ground_truth_list = []\n",
    "for ground_truth_file in ground_truth_files:\n",
    "    ground_truth_data = read_jsonl(ground_truth_file)\n",
    "    ground_truth_list.append(convert_data_for_evaluation(ground_truth_data, name='labels'))\n",
    "\n",
    "print(\"#####Testing Metrics#####\")\n",
    "results = evaluate_aspect_and_sentiment(predictions_list[0], ground_truth_list[0])\n",
    "print(results)\n",
    "\n",
    "\n",
    "# # Test cases\n",
    "# predictions_list = [\n",
    "#     [(\"battery\", \"positive\"), (\"screen\", \"positive\"), (\"performance\", \"positive\")],\n",
    "\n",
    "# ]\n",
    "# ground_truth_list = [\n",
    "#     [(\"battery\", \"positive\"), (\"screen\", \"positive\")],\n",
    "# ]\n",
    "\n",
    "# print(\"TEST TEST TEST\")\n",
    "# results = per_review_evaluate_aspect_and_sentiment(predictions_list, ground_truth_list)\n",
    "# print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
