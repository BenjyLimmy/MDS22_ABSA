{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4cc93cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "Loaded 100 predictions\n",
      "Loaded 816 ground truth reviews\n",
      "\n",
      "Matching predictions to ground truth by text...\n",
      "Successfully matched 100 reviews\n",
      "\n",
      "Running comprehensive evaluation...\n",
      "======================================================================\n",
      "CATEGORY DETECTION EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Dataset Overview:\n",
      "  Total Reviews: 100\n",
      "  Reviews with Mixed Polarity: 4 (4.0%)\n",
      "\n",
      "--------------------------------------------------\n",
      "OVERALL CATEGORY DETECTION METRICS\n",
      "--------------------------------------------------\n",
      "\n",
      "Method 1 - Intersection/Max Accuracy:\n",
      "  Aspect     0.2800\n",
      "  Opinion    0.4667\n",
      "  Category   0.1850\n",
      "  Polarity   0.4600\n",
      "\n",
      "Method 2 - Standard Metrics:\n",
      "Component  Precision  Recall     F1-Score  \n",
      "----------------------------------------\n",
      "Aspect     0.4000     0.0460     0.0825    \n",
      "Opinion    0.5455     0.0811     0.1412    \n",
      "Category   0.3725     0.1743     0.2375    \n",
      "Polarity   0.8704     0.4434     0.5875    \n",
      "\n",
      "Polarity Accuracy (Conditional):\n",
      "  When Aspect Matches:   0.6000\n",
      "  When Opinion Matches:  0.6667\n",
      "  When Category Matches: 0.8636\n",
      "\n",
      "--------------------------------------------------\n",
      "DETAILED STATISTICS\n",
      "--------------------------------------------------\n",
      "\n",
      "Aspect:\n",
      "  Total Predicted: 10\n",
      "  Total Ground Truth: 87\n",
      "  Total Correct: 4\n",
      "\n",
      "Opinion:\n",
      "  Total Predicted: 11\n",
      "  Total Ground Truth: 74\n",
      "  Total Correct: 6\n",
      "\n",
      "Category:\n",
      "  Total Predicted: 51\n",
      "  Total Ground Truth: 109\n",
      "  Total Correct: 19\n",
      "\n",
      "Polarity:\n",
      "  Total Predicted: 54\n",
      "  Total Ground Truth: 106\n",
      "  Total Correct: 47\n",
      "\n",
      "Polarity (Conditional):\n",
      "  Aspect Match - Correct: 3/5\n",
      "  Opinion Match - Correct: 4/6\n",
      "  Category Match - Correct: 19/22\n",
      "\n",
      "Detailed results saved to: absa_evaluation_results.csv\n",
      "Summary saved to: absa_evaluation_summary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_predictions(predictions_filepath):\n",
    "    \"\"\"Load predictions from model output.\"\"\"\n",
    "    with open(predictions_filepath, 'r', encoding='utf-8') as f:\n",
    "        predictions_data = json.load(f)\n",
    "    return predictions_data\n",
    "\n",
    "\n",
    "def load_ground_truth(ground_truth_filepath):\n",
    "    \"\"\"Load ground truth from jsonl file.\"\"\"\n",
    "    ground_truth_list = []\n",
    "    review_texts = []\n",
    "    \n",
    "    with open(ground_truth_filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            review_texts.append(data.get('text', ''))\n",
    "            ground_truth_list.append(data.get('labels', []))\n",
    "    \n",
    "    return ground_truth_list, review_texts\n",
    "\n",
    "\n",
    "def match_by_text(predictions_data, ground_truth_list, review_texts):\n",
    "    \"\"\"Match predictions to ground truth by text content.\"\"\"\n",
    "    text_to_gt = {}\n",
    "    for i, (gt_labels, text) in enumerate(zip(ground_truth_list, review_texts)):\n",
    "        text_to_gt[text] = gt_labels\n",
    "    \n",
    "    matched_predictions = []\n",
    "    matched_ground_truth = []\n",
    "    matched_texts = []\n",
    "    unmatched_count = 0\n",
    "    \n",
    "    for pred_item in predictions_data:\n",
    "        pred_text = pred_item.get('text', '')\n",
    "        if pred_text in text_to_gt:\n",
    "            pred_labels = pred_item.get('labels', pred_item.get('predictions', []))\n",
    "            gt_labels = text_to_gt[pred_text]\n",
    "            \n",
    "            matched_predictions.append(pred_labels)\n",
    "            matched_ground_truth.append(gt_labels)\n",
    "            matched_texts.append(pred_text)\n",
    "        else:\n",
    "            unmatched_count += 1\n",
    "    \n",
    "    if unmatched_count > 0:\n",
    "        print(f\"Warning: {unmatched_count} predictions could not be matched to ground truth\")\n",
    "    \n",
    "    return matched_predictions, matched_ground_truth, matched_texts\n",
    "\n",
    "\n",
    "def extract_component_values(labels, component):\n",
    "    \"\"\"Extract values for a specific component.\"\"\"\n",
    "    values = []\n",
    "    for label in labels:\n",
    "        if isinstance(label, dict):\n",
    "            if component == 'aspect':\n",
    "                value = label.get('aspect', 'NULL')\n",
    "            elif component == 'opinion':\n",
    "                value = label.get('opinion', 'NULL')\n",
    "            elif component == 'category':\n",
    "                value = label.get('category', 'NULL#NULL')\n",
    "            elif component == 'polarity':\n",
    "                value = label.get('polarity', 'neutral')\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if value and value not in ['NULL', 'NULL#NULL', '']:\n",
    "                values.append(value)\n",
    "    \n",
    "    return values\n",
    "\n",
    "\n",
    "def calculate_intersection_max_accuracy(pred_values, true_values):\n",
    "    \"\"\"Calculate intersection/max accuracy for components.\"\"\"\n",
    "    pred_set = set(pred_values)\n",
    "    true_set = set(true_values)\n",
    "    \n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        return 1.0, 0, 0, 0\n",
    "    elif len(true_set) == 0 or len(pred_set) == 0:\n",
    "        return 0.0, 0, len(pred_set), len(true_set)\n",
    "    else:\n",
    "        intersection = len(pred_set.intersection(true_set))\n",
    "        max_size = max(len(pred_set), len(true_set))\n",
    "        return intersection / max_size, intersection, len(pred_set), len(true_set)\n",
    "\n",
    "\n",
    "def evaluate_polarity_given_match(pred_labels, gt_labels, match_component):\n",
    "    \"\"\"\n",
    "    Evaluate polarity accuracy given that a specific component matches.\n",
    "    Returns: (correct_polarities, total_matched_items)\n",
    "    \"\"\"\n",
    "    # Convert labels to dictionaries for easier lookup\n",
    "    pred_dict = {}\n",
    "    for label in pred_labels:\n",
    "        if isinstance(label, dict):\n",
    "            key = label.get(match_component, 'NULL')\n",
    "            if key and key not in ['NULL', 'NULL#NULL', '']:\n",
    "                if key not in pred_dict:\n",
    "                    pred_dict[key] = []\n",
    "                pred_dict[key].append(label.get('polarity', 'neutral'))\n",
    "    \n",
    "    gt_dict = {}\n",
    "    for label in gt_labels:\n",
    "        if isinstance(label, dict):\n",
    "            key = label.get(match_component, 'NULL')\n",
    "            if key and key not in ['NULL', 'NULL#NULL', '']:\n",
    "                if key not in gt_dict:\n",
    "                    gt_dict[key] = []\n",
    "                gt_dict[key].append(label.get('polarity', 'neutral'))\n",
    "    \n",
    "    # Count correct polarities for matched components\n",
    "    correct_polarities = 0\n",
    "    total_matched = 0\n",
    "    \n",
    "    for key in pred_dict:\n",
    "        if key in gt_dict:\n",
    "            # Component matched, now check polarity\n",
    "            pred_polarities = pred_dict[key]\n",
    "            gt_polarities = gt_dict[key]\n",
    "            \n",
    "            # For each ground truth polarity, check if it exists in predictions\n",
    "            for gt_pol in gt_polarities:\n",
    "                total_matched += 1\n",
    "                if gt_pol in pred_polarities:\n",
    "                    correct_polarities += 1\n",
    "    \n",
    "    return correct_polarities, total_matched\n",
    "\n",
    "\n",
    "def comprehensive_evaluation(predictions_list, ground_truth_list, review_texts):\n",
    "    \"\"\"Run comprehensive ABSA evaluation.\"\"\"\n",
    "    # Initialize counters\n",
    "    total_reviews = len(predictions_list)\n",
    "    \n",
    "    # Metrics for all components including polarity (intersection/max)\n",
    "    component_scores = {\n",
    "        'aspect': {'sum_accuracy': 0, 'total_correct': 0, 'total_predicted': 0, 'total_ground_truth': 0},\n",
    "        'opinion': {'sum_accuracy': 0, 'total_correct': 0, 'total_predicted': 0, 'total_ground_truth': 0},\n",
    "        'category': {'sum_accuracy': 0, 'total_correct': 0, 'total_predicted': 0, 'total_ground_truth': 0},\n",
    "        'polarity': {'sum_accuracy': 0, 'total_correct': 0, 'total_predicted': 0, 'total_ground_truth': 0}\n",
    "    }\n",
    "    \n",
    "    # Conditional polarity metrics\n",
    "    polarity_conditional_scores = {\n",
    "        'given_aspect_match': {'correct': 0, 'total': 0},\n",
    "        'given_opinion_match': {'correct': 0, 'total': 0},\n",
    "        'given_category_match': {'correct': 0, 'total': 0}\n",
    "    }\n",
    "    \n",
    "    # Per-review details\n",
    "    per_review_results = []\n",
    "    mixed_polarity_count = 0\n",
    "    \n",
    "    for idx, (pred_labels, gt_labels) in enumerate(zip(predictions_list, ground_truth_list)):\n",
    "        review_result = {\n",
    "            'index': idx,\n",
    "            'text': review_texts[idx][:100] + '...' if len(review_texts[idx]) > 100 else review_texts[idx],\n",
    "            'text_length': len(review_texts[idx])\n",
    "        }\n",
    "        \n",
    "        # Extract ground truth categories and check mixed polarity\n",
    "        gt_categories = []\n",
    "        polarities = set()\n",
    "        for label in gt_labels:\n",
    "            if isinstance(label, dict):\n",
    "                category = label.get('category', 'NULL#NULL')\n",
    "                if category and category not in ['NULL', 'NULL#NULL', '']:\n",
    "                    gt_categories.append(category)\n",
    "                \n",
    "                polarity = label.get('polarity', 'neutral')\n",
    "                if polarity and polarity.lower() not in ['null', 'neutral']:\n",
    "                    polarities.add(polarity.lower())\n",
    "        \n",
    "        review_result['ground_truth_categories'] = '|'.join(gt_categories)\n",
    "        review_result['has_mixed_polarity'] = len(polarities) > 1\n",
    "        \n",
    "        if review_result['has_mixed_polarity']:\n",
    "            mixed_polarity_count += 1\n",
    "        \n",
    "        # Evaluate all components with intersection/max (including polarity)\n",
    "        for component in ['aspect', 'opinion', 'category', 'polarity']:\n",
    "            pred_values = extract_component_values(pred_labels, component)\n",
    "            true_values = extract_component_values(gt_labels, component)\n",
    "            \n",
    "            accuracy, correct, pred_count, true_count = calculate_intersection_max_accuracy(pred_values, true_values)\n",
    "            \n",
    "            component_scores[component]['sum_accuracy'] += accuracy\n",
    "            component_scores[component]['total_correct'] += correct\n",
    "            component_scores[component]['total_predicted'] += pred_count\n",
    "            component_scores[component]['total_ground_truth'] += true_count\n",
    "            \n",
    "            review_result[f'{component}_accuracy'] = accuracy\n",
    "            review_result[f'{component}_pred'] = pred_count\n",
    "            review_result[f'{component}_true'] = true_count\n",
    "            review_result[f'{component}_correct'] = correct\n",
    "        \n",
    "        # Evaluate conditional polarity\n",
    "        for match_type, component in [('given_aspect_match', 'aspect'), \n",
    "                                      ('given_opinion_match', 'opinion'), \n",
    "                                      ('given_category_match', 'category')]:\n",
    "            correct, total = evaluate_polarity_given_match(pred_labels, gt_labels, component)\n",
    "            polarity_conditional_scores[match_type]['correct'] += correct\n",
    "            polarity_conditional_scores[match_type]['total'] += total\n",
    "            \n",
    "            review_result[f'polarity_{match_type}_correct'] = correct\n",
    "            review_result[f'polarity_{match_type}_total'] = total\n",
    "        \n",
    "        per_review_results.append(review_result)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_results = {\n",
    "        'total_reviews': total_reviews,\n",
    "        'reviews_with_mixed_polarity': mixed_polarity_count,\n",
    "        'percentage_mixed_polarity': (mixed_polarity_count / total_reviews * 100) if total_reviews > 0 else 0,\n",
    "        'component_metrics': {},\n",
    "        'polarity_conditional_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Component metrics (all including polarity)\n",
    "    for component in ['aspect', 'opinion', 'category', 'polarity']:\n",
    "        scores = component_scores[component]\n",
    "        avg_accuracy = scores['sum_accuracy'] / total_reviews if total_reviews > 0 else 0\n",
    "        \n",
    "        # Calculate precision, recall, F1\n",
    "        precision = scores['total_correct'] / scores['total_predicted'] if scores['total_predicted'] > 0 else 0\n",
    "        recall = scores['total_correct'] / scores['total_ground_truth'] if scores['total_ground_truth'] > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        final_results['component_metrics'][component] = {\n",
    "            'intersection_max_accuracy': avg_accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'total_correct': scores['total_correct'],\n",
    "            'total_predicted': scores['total_predicted'],\n",
    "            'total_ground_truth': scores['total_ground_truth']\n",
    "        }\n",
    "    \n",
    "    # Conditional polarity metrics\n",
    "    for match_type in ['given_aspect_match', 'given_opinion_match', 'given_category_match']:\n",
    "        scores = polarity_conditional_scores[match_type]\n",
    "        accuracy = scores['correct'] / scores['total'] if scores['total'] > 0 else 0\n",
    "        \n",
    "        final_results['polarity_conditional_metrics'][match_type] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': scores['correct'],\n",
    "            'total': scores['total']\n",
    "        }\n",
    "    \n",
    "    return final_results, per_review_results\n",
    "\n",
    "\n",
    "def print_results(results):\n",
    "    \"\"\"Print evaluation results in a clear format.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CATEGORY DETECTION EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  Total Reviews: {results['total_reviews']}\")\n",
    "    print(f\"  Reviews with Mixed Polarity: {results['reviews_with_mixed_polarity']} ({results['percentage_mixed_polarity']:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"OVERALL CATEGORY DETECTION METRICS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # All components with all metrics\n",
    "    print(\"\\nMethod 1 - Intersection/Max Accuracy:\")\n",
    "    for component in ['aspect', 'opinion', 'category', 'polarity']:\n",
    "        print(f\"  {component.capitalize():<10} {results['component_metrics'][component]['intersection_max_accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"\\nMethod 2 - Standard Metrics:\")\n",
    "    print(f\"{'Component':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for component in ['aspect', 'opinion', 'category', 'polarity']:\n",
    "        metrics = results['component_metrics'][component]\n",
    "        print(f\"{component.capitalize():<10} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} {metrics['f1']:<10.4f}\")\n",
    "    \n",
    "    # Conditional polarity metrics\n",
    "    print(\"\\nPolarity Accuracy (Conditional):\")\n",
    "    print(f\"  When Aspect Matches:   {results['polarity_conditional_metrics']['given_aspect_match']['accuracy']:.4f}\")\n",
    "    print(f\"  When Opinion Matches:  {results['polarity_conditional_metrics']['given_opinion_match']['accuracy']:.4f}\")\n",
    "    print(f\"  When Category Matches: {results['polarity_conditional_metrics']['given_category_match']['accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"DETAILED STATISTICS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Component details\n",
    "    for component in ['aspect', 'opinion', 'category', 'polarity']:\n",
    "        metrics = results['component_metrics'][component]\n",
    "        print(f\"\\n{component.capitalize()}:\")\n",
    "        print(f\"  Total Predicted: {metrics['total_predicted']}\")\n",
    "        print(f\"  Total Ground Truth: {metrics['total_ground_truth']}\")\n",
    "        print(f\"  Total Correct: {metrics['total_correct']}\")\n",
    "    \n",
    "    # Conditional polarity details\n",
    "    print(\"\\nPolarity (Conditional):\")\n",
    "    for match_type, label in [('given_aspect_match', 'Aspect Match'),\n",
    "                             ('given_opinion_match', 'Opinion Match'),\n",
    "                             ('given_category_match', 'Category Match')]:\n",
    "        metrics = results['polarity_conditional_metrics'][match_type]\n",
    "        print(f\"  {label} - Correct: {metrics['correct']}/{metrics['total']}\")\n",
    "\n",
    "\n",
    "def save_detailed_results(results, per_review_results, output_file):\n",
    "    \"\"\"Save detailed results to CSV.\"\"\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\n",
    "            'index', 'text', 'text_length', 'has_mixed_polarity', 'ground_truth_categories',\n",
    "            'aspect_accuracy', 'aspect_correct', 'aspect_pred', 'aspect_true',\n",
    "            'opinion_accuracy', 'opinion_correct', 'opinion_pred', 'opinion_true',\n",
    "            'category_accuracy', 'category_correct', 'category_pred', 'category_true',\n",
    "            'polarity_accuracy', 'polarity_correct', 'polarity_pred', 'polarity_true',\n",
    "            'polarity_given_aspect_match_correct', 'polarity_given_aspect_match_total',\n",
    "            'polarity_given_opinion_match_correct', 'polarity_given_opinion_match_total',\n",
    "            'polarity_given_category_match_correct', 'polarity_given_category_match_total'\n",
    "        ]\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for review in per_review_results:\n",
    "            writer.writerow(review)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the evaluation.\"\"\"\n",
    "    # File paths\n",
    "    predictions_file = \"../datasets/DeepSeek-R1-Distill-Qwen-32B_predictions.json\"\n",
    "    ground_truth_file = \"../datasets/laptop_quad_test.tsv.jsonl\"\n",
    "    \n",
    "    print(\"Loading data files...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        predictions_data = load_predictions(predictions_file)\n",
    "        print(f\"Loaded {len(predictions_data)} predictions\")\n",
    "        \n",
    "        ground_truth_list, review_texts = load_ground_truth(ground_truth_file)\n",
    "        print(f\"Loaded {len(ground_truth_list)} ground truth reviews\")\n",
    "        \n",
    "        # Match by text\n",
    "        print(\"\\nMatching predictions to ground truth by text...\")\n",
    "        matched_preds, matched_gt, matched_texts = match_by_text(\n",
    "            predictions_data, ground_truth_list, review_texts\n",
    "        )\n",
    "        print(f\"Successfully matched {len(matched_preds)} reviews\")\n",
    "        \n",
    "        if len(matched_preds) == 0:\n",
    "            print(\"\\nERROR: No matches found!\")\n",
    "            return\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(\"\\nRunning comprehensive evaluation...\")\n",
    "        results, per_review_results = comprehensive_evaluation(\n",
    "            matched_preds, matched_gt, matched_texts\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print_results(results)\n",
    "        \n",
    "        # Save results\n",
    "        csv_file = \"absa_evaluation_results.csv\"\n",
    "        save_detailed_results(results, per_review_results, csv_file)\n",
    "        print(f\"\\nDetailed results saved to: {csv_file}\")\n",
    "        \n",
    "        json_file = \"absa_evaluation_summary.json\"\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Summary saved to: {json_file}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find file - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
